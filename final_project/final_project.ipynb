{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70cb06b4-f58d-46e0-8f71-b616bcc0512d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Final Project\n",
    "\n",
    "**Group HOMEWORK**. This final project can be collaborative. The maximum members of a group is 2. You can also work by yourself. Please respect the academic integrity. **Remember: if you get caught on cheating, you get F.**\n",
    "\n",
    "## A Introduction to the competition\n",
    "\n",
    "<img src=\"news-sexisme-EN.jpg\" alt=\"drawing\" width=\"380\"/>\n",
    "\n",
    "Sexism is a growing problem online. It can inflict harm on women who are targeted, make online spaces inaccessible and unwelcoming, and perpetuate social asymmetries and injustices. Automated tools are now widely deployed to find, and assess sexist content at scale but most only give classifications for generic, high-level categories, with no further explanation. Flagging what is sexist content and also explaining why it is sexist improves interpretability, trust and understanding of the decisions that automated tools use, empowering both users and moderators.\n",
    "\n",
    "This project is based on SemEval 2023 - Task 10 - Explainable Detection of Online Sexism (EDOS). [Here](https://codalab.lisn.upsaclay.fr/competitions/7124#learn_the_details-overview) you can find a detailed introduction to this task.\n",
    "\n",
    "You only need to complete **TASK A - Binary Sexism Detection: a two-class (or binary) classification where systems have to predict whether a post is sexist or not sexist**. To cut down training time, we only use a subset of the original dataset (5k out of 20k). The dataset can be found in the same folder. \n",
    "\n",
    "Different from our previous homework, this competition gives you great flexibility (and very few hints), you can determine: \n",
    "-  how to preprocess the input text (e.g., remove emoji, remove stopwords, text lemmatization and stemming, etc.);\n",
    "-  which method to use to encode text features (e.g., TF-IDF, N-grams, Word2vec, GloVe, Part-of-Speech (POS), etc.);\n",
    "-  which model to use.\n",
    "\n",
    "## Requirements\n",
    "-  **Input**: the text for each instance.\n",
    "-  **Output**: the binary label for each instance.\n",
    "-  **Feature engineering**: use at least 2 different methods to extract features and encode text into numerical values.\n",
    "-  **Model selection**: implement with at least 3 different models and compare their performance.\n",
    "-  **Evaluation**: create a dataframe with rows indicating feature+model and columns indicating Precision, Accuracy and F1-score (using weighted average). Your results should have at least 6 rows (2 feature engineering methods x 3 models). Report best performance with (1) your feature engineering method, and (2) the model you choose. \n",
    "- **Format**: add explainations for each step (you can add markdown cells). At the end of the report, write a summary and answer the following questions: \n",
    "    - What preprocessing steps do you follow?\n",
    "    - How do you select the features from the inputs? \n",
    "    - Which model you use and what is the structure of your model?\n",
    "    - How do you train your model?\n",
    "    - What is the performance of your best model?\n",
    "    - What other models or feature engineering methods would you like to implement in the future?\n",
    "- **Two Rules**, violations will result in 0 points in the grade: \n",
    "    - Not allowed to use test set in the training: You CANNOT use any of the instances from test set in the training process. \n",
    "    - Not allowed to use code from generative AI (e.g., ChatGPT). \n",
    "\n",
    "## Evaluation\n",
    "\n",
    "The performance should be only evaluated on the test set (a total of 1086 instances). Please split original dataset into train set and test set. The test set should NEVER be used in the training process. The evaluation metric is a combination of precision, recall, and f1-score (use `classification_report` in sklearn). \n",
    "\n",
    "The total points are 10.0. Each team will compete with other teams in the class on their best performance. Points will be deducted if not following the requirements above.\n",
    "\n",
    "If ALL the requirements are met:\n",
    "- Top 25\\% teams: 10.0 points.\n",
    "- Top 25\\% - 50\\% teams: 8.5 points.\n",
    "- Top 50\\% - 75\\% teams: 7.0 points.\n",
    "- Top 75\\% - 100\\% teams: 6.0 points.\n",
    "\n",
    "## Submission\n",
    "Similar as homework, submit both a PDF and .ipynb version of the report. \n",
    "\n",
    "The report should include: (a)code, (b)outputs, (c)explainations for each step, and (d)summary (you can add markdown cells). \n",
    "\n",
    "The due date is **December 8, Friday by 11:59pm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745d219-39aa-4bbb-82a9-4c0b767ef9a9",
   "metadata": {},
   "source": [
    "## Imports\r\n",
    "We include all of the imports that are needed throughout the duration of the program here. This prevents us from needing to import each component separately in the future steps\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b7caf28-1c77-4f87-8a62-7f3a7f79e1c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\very cool\n",
      "[nltk_data]     guy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\very cool\n",
      "[nltk_data]     guy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: <module 'tensorflow._api.v2.version' from 'C:\\\\Users\\\\very cool guy\\\\anaconda3\\\\envs\\\\tf-gpu\\\\lib\\\\site-packages\\\\tensorflow\\\\_api\\\\v2\\\\version\\\\__init__.py'>\n",
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "import re\n",
    "import emoji\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf260f-8a6c-42b0-9bd4-2815861c7314",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Process data\n",
    "In this step we process all of the text. We do so by providing the functionality of lower case conversion, lemmatization as well as removal of emojis, step words and punctuation. This will allow our model to make predictions on our data, as unnecessary components of the message have been processed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "168a3117-bcc4-4e77-a172-2554db7cbe01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lukel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lukel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "df = pd.read_csv('edos_labelled_data.csv')\n",
    "\n",
    "def remove_emojis(string):\n",
    "    return emoji.replace_emoji(string, replace='')\n",
    "\n",
    "def remove_stop_words(string):\n",
    "    words = filter(None, string.split(' '))\n",
    "    retval = []\n",
    "    for w in words:\n",
    "        if not w in stop_words:\n",
    "            retval.append(w)\n",
    "    return \" \".join(retval)\n",
    "    \n",
    "def to_lower_case(value):\n",
    "    return value.lower()\n",
    "\n",
    "def remove_punctuation(string):\n",
    "    return re.sub(r'[^\\w\\s]', '', string)\n",
    "    \n",
    "def lemmatize(string):\n",
    "    words = word_tokenize(string)\n",
    "    stemmer = PorterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in words])\n",
    "\n",
    "label_encoder = LabelEncoder() \n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "\n",
    "df['text'] = df['text'].apply(to_lower_case)\n",
    "df['text'] = df['text'].apply(remove_emojis)\n",
    "df['text'] = df['text'].apply(remove_stop_words)\n",
    "df['text'] = df['text'].apply(remove_punctuation)\n",
    "df['text'] = df['text'].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c34859-0635-4c7b-8a66-b12ed174c672",
   "metadata": {},
   "source": [
    "## Split into train and test\n",
    "This will split up the data into separate testing and training data. This will allow us to properly train our models, as well as run tests on the model which will allow us to gauge their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eee2548e-a7b1-465c-8588-0603b6af9c6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = df[df['split'] == \"train\"]\n",
    "test = df[df['split'] == \"test\"]\n",
    "\n",
    "X_train = train[\"text\"]\n",
    "y_train = train[\"label\"]\n",
    "\n",
    "X_test = test[\"text\"]\n",
    "y_test = test[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f299b09-9951-47c1-a32a-00521149097b",
   "metadata": {},
   "source": [
    "## Import BERT\n",
    "This will bring in the necessary functionality to incorporate the needed components for our BERT encoding as well as provide other needed functionality that will allow us to properly encode our features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed8feb99-d7b6-42a4-b558-1daa35f0b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_preprocess = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "bert_encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2692a18-46ec-4dc8-beaa-7c88d653a24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_embeding(sentences):\n",
    "    preprocessed_text = bert_preprocess(sentences)\n",
    "    return bert_encoder(preprocessed_text)['pooled_output']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a59928-845f-4858-b842-3efe3396e636",
   "metadata": {},
   "source": [
    "## Create model\n",
    "Maybe Remove this section later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64417da9-df18-4000-847a-42abcc47a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert layers\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "preprocessed_text = bert_preprocess(text_input)\n",
    "outputs = bert_encoder(preprocessed_text)\n",
    "\n",
    "# Neural network layers\n",
    "l = tf.keras.layers.Dropout(0.1, name=\"dropout\")(outputs['pooled_output'])\n",
    "l = tf.keras.layers.Dense(1, activation='sigmoid', name=\"output\")(l)\n",
    "\n",
    "# Use inputs and outputs to construct a final model\n",
    "model = tf.keras.Model(inputs=[text_input], outputs = [l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3a1430-84c2-4dfd-bbaf-e35eff407921",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "132/132 [==============================] - 41s 239ms/step - loss: 0.6201 - accuracy: 0.6959 - precision: 0.3333 - recall: 0.0127\n",
      "Epoch 2/10\n",
      "132/132 [==============================] - 31s 238ms/step - loss: 0.6131 - accuracy: 0.6959 - precision: 0.3788 - recall: 0.0199\n",
      "Epoch 3/10\n",
      "132/132 [==============================] - 32s 242ms/step - loss: 0.6033 - accuracy: 0.7007 - precision: 0.5164 - recall: 0.0500\n",
      "Epoch 4/10\n",
      "132/132 [==============================] - 32s 242ms/step - loss: 0.6030 - accuracy: 0.6976 - precision: 0.4602 - recall: 0.0413\n",
      "Epoch 5/10\n",
      "132/132 [==============================] - 32s 245ms/step - loss: 0.5921 - accuracy: 0.6952 - precision: 0.4202 - recall: 0.0397\n",
      "Epoch 6/10\n",
      "132/132 [==============================] - 33s 250ms/step - loss: 0.5967 - accuracy: 0.7014 - precision: 0.5229 - recall: 0.0635\n",
      "Epoch 7/10\n",
      "132/132 [==============================] - 32s 244ms/step - loss: 0.5892 - accuracy: 0.6995 - precision: 0.4966 - recall: 0.0572\n",
      "Epoch 8/10\n",
      "132/132 [==============================] - 33s 247ms/step - loss: 0.5827 - accuracy: 0.7040 - precision: 0.5437 - recall: 0.0890\n",
      "Epoch 9/10\n",
      "132/132 [==============================] - 33s 250ms/step - loss: 0.5826 - accuracy: 0.7076 - precision: 0.5673 - recall: 0.1104\n",
      "Epoch 10/10\n",
      "132/132 [==============================] - 33s 252ms/step - loss: 0.5830 - accuracy: 0.7057 - precision: 0.5519 - recall: 0.1056\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24502638cd0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METRICS = [\n",
    "      tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "      tf.keras.metrics.Precision(name='precision'),\n",
    "      tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=METRICS)\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ae95003-9601-4f1e-a87c-3ba9aedd820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [==============================] - 9s 243ms/step - loss: 0.5658 - accuracy: 0.7505 - precision: 0.5956 - recall: 0.2727\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5658291578292847,\n",
       " 0.7504603862762451,\n",
       " 0.595588207244873,\n",
       " 0.27272728085517883]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a7ea03",
   "metadata": {},
   "source": [
    "## Create the Results DataFrame\n",
    "This will create the initial dataframe that contains all of the columns representing the performance of a given model. It also provides the functionality of a function which will append a new row of data into the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "337cf2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['Feature And Model', 'Precision', 'Accuracy', 'F1-score']\n",
    "results = pd.DataFrame(columns=columns) \n",
    "\n",
    "def add_new_result(results_df, report, model_name, accuracy):\n",
    "    precision = report['weighted avg']['precision']\n",
    "    f1_score = report['weighted avg']['f1-score']\n",
    "    data = {\n",
    "        'Feature And Model': model_name, \n",
    "        'Precision': precision, \n",
    "        'Accuracy': accuracy, \n",
    "        'F1-score': f1_score\n",
    "    }\n",
    "    row_data = pd.DataFrame([data])\n",
    "    results_df = pd.concat([results_df, row_data])\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4362406-6024-45b8-8594-082d3edd51fe",
   "metadata": {},
   "source": [
    "## Naive Bayes TF-IDF\n",
    "This will first use TF-IDF, in order to properly encode the data into its correct form. Once encoded it will pass with data into a Naive Bayes model for training. Then it will run the testing on the model to gauge how well it performed and place the performance into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c78bc58-eda3-4f82-8b18-3d24fb5a69fb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\very cool guy\\AppData\\Local\\Temp\\ipykernel_17636\\1244631138.py:14: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, row_data])\n"
     ]
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('naive bayes', MultinomialNB())         \n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "nb_predictions = clf.predict(X_test)\n",
    "report = classification_report(y_test, nb_predictions, output_dict=True)\n",
    "accuracy = accuracy_score(y_test, nb_predictions)\n",
    "results = add_new_result(results, report, \"Naive Bayes With TF-IDF\", accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2438cdc4-fc09-4abb-9c33-f6f263ceddcc",
   "metadata": {},
   "source": [
    "## Naive Bayes BERT\n",
    "This will first use BERT, in order to properly encode the data into its correct form. Once encoded it will pass with data into a Naive Bayes model for training. Then it will run the testing on the model to gauge how well it performed and place the performance into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "051ad639-b3e2-4c42-8c11-e7bf3ca11dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fed4fa9-1329-409e-a106-7b8e3fb9b2df",
   "metadata": {},
   "source": [
    "## Support Vector Machine TF-IDF\n",
    "This will first use TF-IDF, in order to properly encode the data into its correct form. Once encoded it will pass with data into a Support Vector Machine model for training. Then it will run the testing on the model to gauge how well it performed and place the performance into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c593dab4-e816-44c3-befd-8e99c937d737",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('SVC', SVC(kernel='linear'))         \n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "svm_predictions = clf.predict(X_test)\n",
    "report = classification_report(y_test, svm_predictions, output_dict=True)\n",
    "accuracy = accuracy_score(y_test, svm_predictions)\n",
    "results = add_new_result(results, report, \"Support Vector machine With TF-IDF\", accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd5b80e-5960-4117-a7b6-bf3d9dfb1ec1",
   "metadata": {},
   "source": [
    "## Support Vector Machine BERT\n",
    "This will first use BERT, in order to properly encode the data into its correct form. Once encoded it will pass with data into a Support Vector Machine model for training. Then it will run the testing on the model to gauge how well it performed and place the performance into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cf07620-9e94-4c69-a495-b4abf1698b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d302708c-5e32-46df-ac0e-0d6010d30582",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors TF-IDF\n",
    "This will first use TF-IDF, in order to properly encode the data into its correct form. Once encoded it will pass with data into a K Nearest Neighbors model for training. Then it will run the testing on the model to gauge how well it performed and place the performance into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b24cf76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "     ('vectorizer_tfidf',TfidfVectorizer()),    \n",
    "     ('knn', KNeighborsClassifier(n_neighbors=3))         \n",
    "])\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "knn_predictions = clf.predict(X_test)\n",
    "report = classification_report(y_test, knn_predictions, output_dict=True)\n",
    "accuracy = accuracy_score(y_test, knn_predictions)\n",
    "results = add_new_result(results, report, \"K Nearest Neighbors With TF-IDF\", accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e402541e-b7d0-4e36-a3d3-a979602f3af2",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors BERT\n",
    "This will first use TF-IDF, in order to properly encode the data into its correct form. Once encoded it will pass with data into a K Nearest Neighbors model for training. Then it will run the testing on the model to gauge how well it performed and place the performance into the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9ba57968-b6fa-4091-aac1-62b9b1b17548",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2d375-64b2-4603-8708-4d2cf9d1011f",
   "metadata": {},
   "source": [
    "## Results Display \n",
    "This will display the results of all the models and encododing so that it can be easily seen what the performance of each is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "96e7eda2-875b-4870-8ee2-40b07bb7951a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature And Model</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1-score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Naive Bayes With TF-IDF</td>\n",
       "      <td>0.792938</td>\n",
       "      <td>0.740331</td>\n",
       "      <td>0.644122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Support Vector machine With TF-IDF</td>\n",
       "      <td>0.813795</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.799877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K Nearest Neighbors With TF-IDF</td>\n",
       "      <td>0.684964</td>\n",
       "      <td>0.729282</td>\n",
       "      <td>0.672106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>K Nearest Neighbors With TF-IDF</td>\n",
       "      <td>0.684964</td>\n",
       "      <td>0.729282</td>\n",
       "      <td>0.672106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature And Model  Precision  Accuracy  F1-score\n",
       "0             Naive Bayes With TF-IDF   0.792938  0.740331  0.644122\n",
       "0  Support Vector machine With TF-IDF   0.813795  0.817680  0.799877\n",
       "0     K Nearest Neighbors With TF-IDF   0.684964  0.729282  0.672106\n",
       "0     K Nearest Neighbors With TF-IDF   0.684964  0.729282  0.672106"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7827b394-47a9-4ccc-8ee3-9a9059064cff",
   "metadata": {},
   "source": [
    "### Summary\r\n",
    "\r\n",
    "1. **What preprocessing steps do you follow?**  \r\n",
    "   Our initial preprocessing began with us removing the emojis from the text using a Python library which would detect when a character was an emoji. Then we eliminated all of the stop words, thus filtering out common words that hold very little relevance in deciding whether a message contains sexist content. We also removed all punctuation and converted all characters to lowercase. Lastly, we performed lemmatization on the message to make words with similar form the exact same to standardized the word.\r\n",
    "\r\n",
    "2. **How do you select the features from the inputs?**  \r\n",
    "   We began by extracting messages from the CSV file and preprocessing the text for feature selection. Since the models struggle in comprehending text, we decided to convert it into numerical values using two different methods: tf-idf and BERT encoding. The tf-idf method assigns weights to individual terms by prioritizing less frequent words such as \"ugly,\" thus amplifying their importance, whereas more common words will hold less weight. Meanwhile, BERT encoding uses contextual understanding for sentences along with being pretrained on vast amounts of text.\r\n",
    "\r\n",
    "3. **Which model do you use and what is the structure of your model?**  \r\n",
    "   We tried various models which included Naive Bayes, Support Vector Machine, and K Nearest Neighbors to ensure we selected the best model for classifying a message as sexist or not.\r\n",
    "   - **Naive Bayes:** This uses the specific probabilities of each term to make a prediction. For example, terms like “hate” are going to occur more frequently in sexist messages than in non-sexist messages. So the model will run a calculation on each term in a provided message to determine the likelihood of the message being sexist or not. Then it will choose the labeling of the message which has the higher probability.\r\n",
    "   - **Support Vector Machines:** Works by plotting a hyperplane using all of the training data. Then, in order to determine if a message is sexist or not, it will plot a point of the provided message and it will use its position relative to the hyperplane to determine if it is sexist or not.\r\n",
    "   - **K Nearest Neighbors:** Creates a vector space from the training messages. Then the model will predict if a text message is sexist or not by looking at the n closest vectors to the test vector. The test vector will be classified based on what most of these neighboring training vectors are.\r\n",
    "\r\n",
    "4. **How do you train your model?**  \r\n",
    "   To train our model, we simply had to pass our processed training data to the model. Scikit-learn was then able to do the specific implementations of the training in the function call itself. This allowed us to then be able to call and make predictions on the model in order to determine if a message is sexist or not.\r\n",
    "\r\n",
    "5. **What is the performance of your best model?**  \r\n",
    "   Overall, our performance was by far the best when using the Support Vector Machine. We found that when using the tf-idf method we were able to achieve an accuracy of around 0.82, whereas with the BERT method we were able to achieve an accuracy of xxx. This was 6 - 8% better performing than all of our other models making it the best model in our prediction of if a message is sexist or not.\r\n",
    "\r\n",
    "6. **What other models or feature engineering methods would you like to implement in the future?**  \r\n",
    "   In the future, it would be nice to implement a neural network for a model in order to determine whether a message is sexist or not. Message classification is often done very well by a neural network, so this would likely give us a much higher prediction accuracy. We had chosen not to do this method due to complexity with understanding how to properly change parameters to make the model better. So as we continue to grow more in our understanding of how these neural networks work, we will likely want to implement it.\r\n",
    "\r\n",
    "**Overview:**  \r\n",
    "Through this project, we were able to properly train a variety of models to predict if a message was sexist or not. Prior to training the models, we had to process our data so that they would be in a format that our model could make accurate predictions on. We processed our data by first changing the messages to be in a useful form. We did so through the use of lowercase conversion, lemmatization as well as removal of emojis, stop words, and punctuation. Afterwards, we needed to have our text representations turned into numerical values so that the models could be properly trained with the data. We did this encoding through the use of BERT and tf-idf. Once encoded, we used our data to train three separate models: Naive Bayes, Support Vector Machine, and K Nearest Neighbors. From training these models, we were able to achieve high success with the Support Vector Machine.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58aae1b4-61da-4f66-8aea-c87dc0ad77fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
